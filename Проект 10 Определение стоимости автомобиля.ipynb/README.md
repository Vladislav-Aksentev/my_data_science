# Разработка алгоритма для защиты данных.

Задача - разработать модель с наилучшим балансом обучния(время и метрика RMSE) и предсказания.  

### Анализ моделей:

- Линейная регрессия:  
На валидационной выборке наилучшее качество получилось с параметрами iters = 2 и lerning_rate = 1.5 rmse = 3059.0. Качество на тестовой выборке с этими параметраметрами, соответсвенно тоже наилучшее = 3032.0.  

Тестовые результаты:  
Время обучения с 2 итерациями с шагом 1 = 1min 3s. Время предсказания = 418 ms. Метрика RMSE = 6216.0.  
Время обучения с 3 итерациями с шагом 1 = 1min 40s. Время предсказания = 782 ms. Метрика RMSE = 6214.0.  
Время обучения с 20 итерациями с шагом 1 = 7min 50s. Время предсказания =  2.8 s. Метрика RMSE = 6214.0.  
Качество на простой линейной регрессии при этом всеравно выше.  


- КэтБуст:
Наилучшее качество на кросс валидации получилось с глубиной 11 = 2889. Проверка на тестовой выборке подтвердила, что с такой глубиной среди выбранных, получается наилучшее качество.  

Время обучения с глубиной дерева 2 = 18.7 s. Время предсказания = 154 ms. Метрика RMSE = 2432.0  
Время обучения с глубиной дерева 8 = 48.2 s. Время предсказания = 152 ms. Метрика RMSE = 1855.0  
Время обучения с глубиной дерева 11 = 1 min 3 s. Время предсказания = 234 ms. Метрика RMSE = 1770.0  


- ЛайтДжиБиЭм:  
Наилучшее качество на кросс валидации получилось с параметрками learning_rate = 0.3 и n_estimators = 150 = 1602.5. С такими параметрами на тестовой выборке, получился результат 1578.0, время обучения 32.3 s, время предсказания 795 ms.  

Время обучения с шагом 0.5 количеством деревьев 50 = 3.77 s. Время предсказания = 290 ms. Метрика RMSE = 1637.0  
Время обучения с шагом 0.3 количеством деревьев 100 = 7.38 s. Время предсказания = 402 ms. Метрика RMSE = 1598.0  
Время обучения с шагом 0.1 количеством деревьев 150 = 26.4 s. Время предсказания = 992 ms. Метрика RMSE = 1623.0  
Время обучения с шагом 0.1 количеством деревьев 200 = 36.3 s. Время предсказания = 1.32 s. Метрика RMSE = 1602.0 

Можно сделать вывод, что У всех моделей значительно отличается время обучения, а также и время предсказаний при изменении гиперпараметров. в Некоторых случаях заметно переобучение моделей, когда РМСЕ увеличивалась при лучших гипперпараметрах. Линейная регрессия дала наихудший результат за очень большое время. Если точность результата не слишком критична, можно значительно сократить время при не слишком сильных различаях РМСЕ.Например - Модель LGBM. При шаге 0.5 и 50 дереьях против 0.3 шага и 100 деревьев,  разница РМСЕ = 39(1637-1598), временя обучения различается почти в 2 раза. Далее шаг 0.1 и 150 деревьев - получили ухудшение метрики, а вот время обучения выросло почти в 5 раз. При шаге 0.1 и 200 деревьях метрика вернулась к лучшему показателю, но время обучения увеличилось уже в 6 раз от первоначального. Следовательно, при не большой потере в метрике, получаем очень быстро обучаемую модель.  
### Статус проекта:

Проект сдан.  

### Библиотеки, используемые в проекте:

pandas  
numpy  

sklearn.model_selection.train_test_split  
sklearn.metrics.mean_squared_error  
sklearn.preprocessing.OneHotEncoder  
sklearn.preprocessing.OrdinalEncoder  
sklearn.preprocessing.StandardScaler  
sklearn.linear_model.LinearRegression  
sklearn.metrics.make_scorer  
sklearn.model_selection.GridSearchCV  

lightgbm.LGBMRegressor  

xgboostxgb  

catboost.CatBoostRegressor  
catboost.Pool  
catboost.cv  
